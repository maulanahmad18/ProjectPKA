{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install feedparser scikit-learn numpy pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XM2tWYZwWGfh",
        "outputId": "7aa2d67a-ea07-45ce-86af-6356f5f56564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.10/dist-packages (6.0.11)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser) (1.0.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import feedparser\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "def search_arxiv(keyword, start=0, max_results=1000):\n",
        "    \"\"\"\n",
        "    Mengambil data paper dari ArXiv dengan kategori CS spesifik\n",
        "    \"\"\"\n",
        "    print(f\"Mencari paper dengan keyword: {keyword} (start: {start}, max: {max_results})\")\n",
        "\n",
        "    base_url = 'http://export.arxiv.org/api/query?'\n",
        "    # Menggunakan kategori CS spesifik\n",
        "    query = f'search_query=all:{keyword}&start={start}&max_results={max_results}&sortBy=submittedDate&sortOrder=descending'\n",
        "    query = query.replace(' ', '+')\n",
        "    url = base_url + query\n",
        "\n",
        "    try:\n",
        "        response = urllib.request.urlopen(url)\n",
        "        feed = feedparser.parse(response.read())\n",
        "\n",
        "        papers = []\n",
        "        for entry in feed.entries:\n",
        "            # Ambil kategori\n",
        "            categories = [t['term'] for t in entry.tags] if 'tags' in entry else []\n",
        "\n",
        "            paper = {\n",
        "                'title': entry.title,\n",
        "                'authors': ', '.join(author.name for author in entry.authors),\n",
        "                'published_date': entry.published,\n",
        "                'summary': entry.summary,\n",
        "                'link': entry.link,\n",
        "                'categories': ', '.join(categories),\n",
        "                'main_category': entry.arxiv_primary_category['term'] if 'arxiv_primary_category' in entry else ''\n",
        "            }\n",
        "            papers.append(paper)\n",
        "\n",
        "        return papers\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    # Keywords berdasarkan kategori CS yang populer\n",
        "    keywords_by_category = {\n",
        "        'Machine Learning & AI': [\n",
        "            'deep learning neural networks',\n",
        "            'reinforcement learning',\n",
        "            'machine learning algorithms',\n",
        "            'natural language processing',\n",
        "            'computer vision recognition'\n",
        "        ],\n",
        "        'Data Science': [\n",
        "            'big data analytics',\n",
        "            'data mining techniques',\n",
        "            'predictive modeling',\n",
        "            'statistical learning'\n",
        "        ],\n",
        "        'Software Engineering': [\n",
        "            'software development methodology',\n",
        "            'agile development',\n",
        "            'software testing',\n",
        "            'code analysis'\n",
        "        ],\n",
        "        'Security': [\n",
        "            'cybersecurity methods',\n",
        "            'network security',\n",
        "            'cryptography algorithms',\n",
        "            'blockchain technology'\n",
        "        ],\n",
        "        'Systems': [\n",
        "            'distributed systems',\n",
        "            'cloud computing architecture',\n",
        "            'operating systems',\n",
        "            'embedded systems design'\n",
        "        ],\n",
        "        'Networks': [\n",
        "            'computer networks protocols',\n",
        "            'wireless networking',\n",
        "            'network optimization',\n",
        "            'internet protocols'\n",
        "        ],\n",
        "        'Database': [\n",
        "            'database management systems',\n",
        "            'query optimization',\n",
        "            'data warehouse',\n",
        "            'NoSQL databases'\n",
        "        ],\n",
        "        'Graphics & Vision': [\n",
        "            '3D computer graphics',\n",
        "            'image processing algorithms',\n",
        "            'virtual reality systems',\n",
        "            'augmented reality'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    all_papers = []\n",
        "    papers_per_keyword = 100  # Mengambil 25 paper per keyword\n",
        "\n",
        "    for category, keywords in keywords_by_category.items():\n",
        "        print(f\"\\nMengambil data untuk kategori: {category}\")\n",
        "        for keyword in keywords:\n",
        "            # Mengambil data dalam satu batch\n",
        "            papers = search_arxiv(keyword, start=0, max_results=papers_per_keyword)\n",
        "\n",
        "            if papers:\n",
        "                all_papers.extend(papers)\n",
        "                print(f\"Berhasil mengambil {len(papers)} paper untuk keyword '{keyword}'\")\n",
        "                time.sleep(3)  # Delay antara requests\n",
        "            else:\n",
        "                print(f\"Tidak ada hasil untuk keyword '{keyword}'\")\n",
        "\n",
        "    if all_papers:\n",
        "        # Hapus duplikat berdasarkan judul\n",
        "        df = pd.DataFrame(all_papers)\n",
        "        df = df.drop_duplicates(subset=['title'])\n",
        "\n",
        "        # Tambah kolom kategori untuk memudahkan analisis\n",
        "        df['timestamp'] = pd.to_datetime(df['published_date'])\n",
        "\n",
        "        # Simpan dan tampilkan hasil\n",
        "        filename = f'arxiv_cs_papers_{datetime.now().strftime(\"%Y%m%d_%H%M\")}.csv'\n",
        "        df.to_csv(filename, index=False)\n",
        "\n",
        "        print(\"\\nContoh data yang berhasil diambil:\")\n",
        "        display(df.head())\n",
        "\n",
        "        print(f\"\\nTotal paper yang terkumpul (setelah menghapus duplikat): {len(df)}\")\n",
        "\n",
        "        # Tampilkan statistik kategori\n",
        "        print(\"\\nDistribusi kategori utama:\")\n",
        "        print(df['main_category'].value_counts().head(10))\n",
        "\n",
        "        # Tampilkan statistik waktu publikasi\n",
        "        print(\"\\nDistribusi tahun publikasi:\")\n",
        "        print(df['timestamp'].dt.year.value_counts().sort_index().tail())\n",
        "\n",
        "        return df\n",
        "    else:\n",
        "        print(\"Tidak ada data yang berhasil diambil\")\n",
        "\n",
        "# Jalankan program\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Memulai pengambilan data dari ArXiv...\")\n",
        "    df = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VY_gcxXCWLCh",
        "outputId": "12d5cabc-25f5-44dc-8b18-7be6bb38b469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memulai pengambilan data dari ArXiv...\n",
            "\n",
            "Mengambil data untuk kategori: Machine Learning & AI\n",
            "Mencari paper dengan keyword: deep learning neural networks (start: 0, max: 100)\n",
            "Berhasil mengambil 100 paper untuk keyword 'deep learning neural networks'\n",
            "Mencari paper dengan keyword: reinforcement learning (start: 0, max: 100)\n",
            "Berhasil mengambil 100 paper untuk keyword 'reinforcement learning'\n",
            "Mencari paper dengan keyword: machine learning algorithms (start: 0, max: 100)\n",
            "Berhasil mengambil 100 paper untuk keyword 'machine learning algorithms'\n",
            "Mencari paper dengan keyword: natural language processing (start: 0, max: 100)\n",
            "Berhasil mengambil 100 paper untuk keyword 'natural language processing'\n",
            "Mencari paper dengan keyword: computer vision recognition (start: 0, max: 100)\n",
            "Berhasil mengambil 100 paper untuk keyword 'computer vision recognition'\n",
            "\n",
            "Mengambil data untuk kategori: Data Science\n",
            "Mencari paper dengan keyword: big data analytics (start: 0, max: 100)\n",
            "Berhasil mengambil 100 paper untuk keyword 'big data analytics'\n",
            "Mencari paper dengan keyword: data mining techniques (start: 0, max: 100)\n",
            "Berhasil mengambil 100 paper untuk keyword 'data mining techniques'\n",
            "Mencari paper dengan keyword: predictive modeling (start: 0, max: 100)\n",
            "Berhasil mengambil 10 paper untuk keyword 'predictive modeling'\n",
            "Mencari paper dengan keyword: statistical learning (start: 0, max: 100)\n",
            "Berhasil mengambil 100 paper untuk keyword 'statistical learning'\n",
            "\n",
            "Mengambil data untuk kategori: Software Engineering\n",
            "Mencari paper dengan keyword: software development methodology (start: 0, max: 100)\n",
            "Berhasil mengambil 100 paper untuk keyword 'software development methodology'\n",
            "Mencari paper dengan keyword: agile development (start: 0, max: 100)\n",
            "Berhasil mengambil 100 paper untuk keyword 'agile development'\n",
            "Mencari paper dengan keyword: software testing (start: 0, max: 100)\n",
            "Berhasil mengambil 30 paper untuk keyword 'software testing'\n",
            "Mencari paper dengan keyword: code analysis (start: 0, max: 100)\n",
            "Berhasil mengambil 100 paper untuk keyword 'code analysis'\n",
            "\n",
            "Mengambil data untuk kategori: Security\n",
            "Mencari paper dengan keyword: cybersecurity methods (start: 0, max: 100)\n",
            "Berhasil mengambil 100 paper untuk keyword 'cybersecurity methods'\n",
            "Mencari paper dengan keyword: network security (start: 0, max: 100)\n",
            "Berhasil mengambil 100 paper untuk keyword 'network security'\n",
            "Mencari paper dengan keyword: cryptography algorithms (start: 0, max: 100)\n",
            "Berhasil mengambil 100 paper untuk keyword 'cryptography algorithms'\n",
            "Mencari paper dengan keyword: blockchain technology (start: 0, max: 100)\n",
            "Berhasil mengambil 100 paper untuk keyword 'blockchain technology'\n",
            "\n",
            "Mengambil data untuk kategori: Systems\n",
            "Mencari paper dengan keyword: distributed systems (start: 0, max: 100)\n",
            "Berhasil mengambil 100 paper untuk keyword 'distributed systems'\n",
            "Mencari paper dengan keyword: cloud computing architecture (start: 0, max: 100)\n",
            "Berhasil mengambil 100 paper untuk keyword 'cloud computing architecture'\n",
            "Mencari paper dengan keyword: operating systems (start: 0, max: 100)\n",
            "Berhasil mengambil 100 paper untuk keyword 'operating systems'\n",
            "Mencari paper dengan keyword: embedded systems design (start: 0, max: 100)\n",
            "Berhasil mengambil 50 paper untuk keyword 'embedded systems design'\n",
            "\n",
            "Mengambil data untuk kategori: Networks\n",
            "Mencari paper dengan keyword: computer networks protocols (start: 0, max: 100)\n",
            "Berhasil mengambil 50 paper untuk keyword 'computer networks protocols'\n",
            "Mencari paper dengan keyword: wireless networking (start: 0, max: 100)\n",
            "Berhasil mengambil 100 paper untuk keyword 'wireless networking'\n",
            "Mencari paper dengan keyword: network optimization (start: 0, max: 100)\n",
            "Berhasil mengambil 100 paper untuk keyword 'network optimization'\n",
            "Mencari paper dengan keyword: internet protocols (start: 0, max: 100)\n",
            "Berhasil mengambil 100 paper untuk keyword 'internet protocols'\n",
            "\n",
            "Mengambil data untuk kategori: Database\n",
            "Mencari paper dengan keyword: database management systems (start: 0, max: 100)\n",
            "Berhasil mengambil 50 paper untuk keyword 'database management systems'\n",
            "Mencari paper dengan keyword: query optimization (start: 0, max: 100)\n",
            "Berhasil mengambil 100 paper untuk keyword 'query optimization'\n",
            "Mencari paper dengan keyword: data warehouse (start: 0, max: 100)\n",
            "Berhasil mengambil 80 paper untuk keyword 'data warehouse'\n",
            "Mencari paper dengan keyword: NoSQL databases (start: 0, max: 100)\n",
            "Berhasil mengambil 100 paper untuk keyword 'NoSQL databases'\n",
            "\n",
            "Mengambil data untuk kategori: Graphics & Vision\n",
            "Mencari paper dengan keyword: 3D computer graphics (start: 0, max: 100)\n",
            "Berhasil mengambil 50 paper untuk keyword '3D computer graphics'\n",
            "Mencari paper dengan keyword: image processing algorithms (start: 0, max: 100)\n",
            "Berhasil mengambil 100 paper untuk keyword 'image processing algorithms'\n",
            "Mencari paper dengan keyword: virtual reality systems (start: 0, max: 100)\n",
            "Berhasil mengambil 50 paper untuk keyword 'virtual reality systems'\n",
            "Mencari paper dengan keyword: augmented reality (start: 0, max: 100)\n",
            "Berhasil mengambil 100 paper untuk keyword 'augmented reality'\n",
            "\n",
            "Contoh data yang berhasil diambil:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                               title  \\\n",
              "0  Retrieving Semantics from the Deep: an RAG Sol...   \n",
              "1  P3-PO: Prescriptive Point Priors for Visuo-Spa...   \n",
              "2  CARP: Visuomotor Policy Learning via Coarse-to...   \n",
              "3  Around the World in 80 Timesteps: A Generative...   \n",
              "4  Driv3R: Learning Dense 4D Reconstruction for A...   \n",
              "\n",
              "                                             authors        published_date  \\\n",
              "0  M. Hamza Mughal, Rishabh Dabral, Merel C. J. S...  2024-12-09T18:59:46Z   \n",
              "1  Mara Levy, Siddhant Haldar, Lerrel Pinto, Abhi...  2024-12-09T18:59:42Z   \n",
              "2  Zhefei Gong, Pengxiang Ding, Shangke Lyu, Site...  2024-12-09T18:59:18Z   \n",
              "3  Nicolas Dufour, David Picard, Vicky Kalogeiton...  2024-12-09T18:59:04Z   \n",
              "4  Xin Fei, Wenzhao Zheng, Yueqi Duan, Wei Zhan, ...  2024-12-09T18:58:03Z   \n",
              "\n",
              "                                             summary  \\\n",
              "0  Non-verbal communication often comprises of se...   \n",
              "1  Developing generalizable robot policies that c...   \n",
              "2  In robotic visuomotor policy learning, diffusi...   \n",
              "3  Global visual geolocation predicts where an im...   \n",
              "4  Realtime 4D reconstruction for dynamic scenes ...   \n",
              "\n",
              "                                link                  categories  \\\n",
              "0  http://arxiv.org/abs/2412.06786v1                       cs.CV   \n",
              "1  http://arxiv.org/abs/2412.06784v1  cs.RO, cs.AI, cs.CV, cs.LG   \n",
              "2  http://arxiv.org/abs/2412.06782v1                cs.RO, cs.CV   \n",
              "3  http://arxiv.org/abs/2412.06781v1                cs.CV, cs.LG   \n",
              "4  http://arxiv.org/abs/2412.06777v1         cs.CV, cs.AI, cs.LG   \n",
              "\n",
              "  main_category                 timestamp  \n",
              "0         cs.CV 2024-12-09 18:59:46+00:00  \n",
              "1         cs.RO 2024-12-09 18:59:42+00:00  \n",
              "2         cs.RO 2024-12-09 18:59:18+00:00  \n",
              "3         cs.CV 2024-12-09 18:59:04+00:00  \n",
              "4         cs.CV 2024-12-09 18:58:03+00:00  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-23a69097-4ef0-46e6-ac62-d32c4e04c395\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>authors</th>\n",
              "      <th>published_date</th>\n",
              "      <th>summary</th>\n",
              "      <th>link</th>\n",
              "      <th>categories</th>\n",
              "      <th>main_category</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Retrieving Semantics from the Deep: an RAG Sol...</td>\n",
              "      <td>M. Hamza Mughal, Rishabh Dabral, Merel C. J. S...</td>\n",
              "      <td>2024-12-09T18:59:46Z</td>\n",
              "      <td>Non-verbal communication often comprises of se...</td>\n",
              "      <td>http://arxiv.org/abs/2412.06786v1</td>\n",
              "      <td>cs.CV</td>\n",
              "      <td>cs.CV</td>\n",
              "      <td>2024-12-09 18:59:46+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>P3-PO: Prescriptive Point Priors for Visuo-Spa...</td>\n",
              "      <td>Mara Levy, Siddhant Haldar, Lerrel Pinto, Abhi...</td>\n",
              "      <td>2024-12-09T18:59:42Z</td>\n",
              "      <td>Developing generalizable robot policies that c...</td>\n",
              "      <td>http://arxiv.org/abs/2412.06784v1</td>\n",
              "      <td>cs.RO, cs.AI, cs.CV, cs.LG</td>\n",
              "      <td>cs.RO</td>\n",
              "      <td>2024-12-09 18:59:42+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CARP: Visuomotor Policy Learning via Coarse-to...</td>\n",
              "      <td>Zhefei Gong, Pengxiang Ding, Shangke Lyu, Site...</td>\n",
              "      <td>2024-12-09T18:59:18Z</td>\n",
              "      <td>In robotic visuomotor policy learning, diffusi...</td>\n",
              "      <td>http://arxiv.org/abs/2412.06782v1</td>\n",
              "      <td>cs.RO, cs.CV</td>\n",
              "      <td>cs.RO</td>\n",
              "      <td>2024-12-09 18:59:18+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Around the World in 80 Timesteps: A Generative...</td>\n",
              "      <td>Nicolas Dufour, David Picard, Vicky Kalogeiton...</td>\n",
              "      <td>2024-12-09T18:59:04Z</td>\n",
              "      <td>Global visual geolocation predicts where an im...</td>\n",
              "      <td>http://arxiv.org/abs/2412.06781v1</td>\n",
              "      <td>cs.CV, cs.LG</td>\n",
              "      <td>cs.CV</td>\n",
              "      <td>2024-12-09 18:59:04+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Driv3R: Learning Dense 4D Reconstruction for A...</td>\n",
              "      <td>Xin Fei, Wenzhao Zheng, Yueqi Duan, Wei Zhan, ...</td>\n",
              "      <td>2024-12-09T18:58:03Z</td>\n",
              "      <td>Realtime 4D reconstruction for dynamic scenes ...</td>\n",
              "      <td>http://arxiv.org/abs/2412.06777v1</td>\n",
              "      <td>cs.CV, cs.AI, cs.LG</td>\n",
              "      <td>cs.CV</td>\n",
              "      <td>2024-12-09 18:58:03+00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-23a69097-4ef0-46e6-ac62-d32c4e04c395')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-23a69097-4ef0-46e6-ac62-d32c4e04c395 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-23a69097-4ef0-46e6-ac62-d32c4e04c395');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1eb9857a-5f39-4a69-a7e9-a1f6ea984777\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1eb9857a-5f39-4a69-a7e9-a1f6ea984777')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1eb9857a-5f39-4a69-a7e9-a1f6ea984777 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    df = main()\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"P3-PO: Prescriptive Point Priors for Visuo-Spatial Generalization of\\n  Robot Policies\",\n          \"Driv3R: Learning Dense 4D Reconstruction for Autonomous Driving\",\n          \"CARP: Visuomotor Policy Learning via Coarse-to-Fine Autoregressive\\n  Prediction\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Mara Levy, Siddhant Haldar, Lerrel Pinto, Abhinav Shirivastava\",\n          \"Xin Fei, Wenzhao Zheng, Yueqi Duan, Wei Zhan, Masayoshi Tomizuka, Kurt Keutzer, Jiwen Lu\",\n          \"Zhefei Gong, Pengxiang Ding, Shangke Lyu, Siteng Huang, Mingyang Sun, Wei Zhao, Zhaoxin Fan, Donglin Wang\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"published_date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2024-12-09T18:59:42Z\",\n          \"2024-12-09T18:58:03Z\",\n          \"2024-12-09T18:59:18Z\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Developing generalizable robot policies that can robustly handle varied\\nenvironmental conditions and object instances remains a fundamental challenge\\nin robot learning. While considerable efforts have focused on collecting large\\nrobot datasets and developing policy architectures to learn from such data,\\nnaively learning from visual inputs often results in brittle policies that fail\\nto transfer beyond the training data. This work presents Prescriptive Point\\nPriors for Policies or P3-PO, a novel framework that constructs a unique state\\nrepresentation of the environment leveraging recent advances in computer vision\\nand robot learning to achieve improved out-of-distribution generalization for\\nrobot manipulation. This representation is obtained through two steps. First, a\\nhuman annotator prescribes a set of semantically meaningful points on a single\\ndemonstration frame. These points are then propagated through the dataset using\\noff-the-shelf vision models. The derived points serve as an input to\\nstate-of-the-art policy architectures for policy learning. Our experiments\\nacross four real-world tasks demonstrate an overall 43% absolute improvement\\nover prior methods when evaluated in identical settings as training. Further,\\nP3-PO exhibits 58% and 80% gains across tasks for new object instances and more\\ncluttered environments respectively. Videos illustrating the robot's\\nperformance are best viewed at point-priors.github.io.\",\n          \"Realtime 4D reconstruction for dynamic scenes remains a crucial challenge for\\nautonomous driving perception. Most existing methods rely on depth estimation\\nthrough self-supervision or multi-modality sensor fusion. In this paper, we\\npropose Driv3R, a DUSt3R-based framework that directly regresses per-frame\\npoint maps from multi-view image sequences. To achieve streaming dense\\nreconstruction, we maintain a memory pool to reason both spatial relationships\\nacross sensors and dynamic temporal contexts to enhance multi-view 3D\\nconsistency and temporal integration. Furthermore, we employ a 4D flow\\npredictor to identify moving objects within the scene to direct our network\\nfocus more on reconstructing these dynamic regions. Finally, we align all\\nper-frame pointmaps consistently to the world coordinate system in an\\noptimization-free manner. We conduct extensive experiments on the large-scale\\nnuScenes dataset to evaluate the effectiveness of our method. Driv3R\\noutperforms previous frameworks in 4D dynamic scene reconstruction, achieving\\n15x faster inference speed compared to methods requiring global alignment.\\nCode: https://github.com/Barrybarry-Smith/Driv3R.\",\n          \"In robotic visuomotor policy learning, diffusion-based models have achieved\\nsignificant success in improving the accuracy of action trajectory generation\\ncompared to traditional autoregressive models. However, they suffer from\\ninefficiency due to multiple denoising steps and limited flexibility from\\ncomplex constraints. In this paper, we introduce Coarse-to-Fine AutoRegressive\\nPolicy (CARP), a novel paradigm for visuomotor policy learning that redefines\\nthe autoregressive action generation process as a coarse-to-fine, next-scale\\napproach. CARP decouples action generation into two stages: first, an action\\nautoencoder learns multi-scale representations of the entire action sequence;\\nthen, a GPT-style transformer refines the sequence prediction through a\\ncoarse-to-fine autoregressive process. This straightforward and intuitive\\napproach produces highly accurate and smooth actions, matching or even\\nsurpassing the performance of diffusion-based policies while maintaining\\nefficiency on par with autoregressive policies. We conduct extensive\\nevaluations across diverse settings, including single-task and multi-task\\nscenarios on state-based and image-based simulation benchmarks, as well as\\nreal-world tasks. CARP achieves competitive success rates, with up to a 10%\\nimprovement, and delivers 10x faster inference compared to state-of-the-art\\npolicies, establishing a high-performance, efficient, and flexible paradigm for\\naction generation in robotic tasks.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"link\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"http://arxiv.org/abs/2412.06784v1\",\n          \"http://arxiv.org/abs/2412.06777v1\",\n          \"http://arxiv.org/abs/2412.06782v1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"categories\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"cs.RO, cs.AI, cs.CV, cs.LG\",\n          \"cs.CV, cs.AI, cs.LG\",\n          \"cs.RO, cs.CV\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"main_category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"cs.RO\",\n          \"cs.CV\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2024-12-09 18:58:03+00:00\",\n        \"max\": \"2024-12-09 18:59:46+00:00\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2024-12-09 18:59:42+00:00\",\n          \"2024-12-09 18:58:03+00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total paper yang terkumpul (setelah menghapus duplikat): 755\n",
            "\n",
            "Distribusi kategori utama:\n",
            "main_category\n",
            "cs.CV             122\n",
            "cs.LG              66\n",
            "quant-ph           42\n",
            "cs.CL              32\n",
            "cs.CR              30\n",
            "cs.DB              28\n",
            "eess.SP            20\n",
            "eess.IV            19\n",
            "cs.AI              19\n",
            "physics.optics     18\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribusi tahun publikasi:\n",
            "timestamp\n",
            "2024    755\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tampilkan jumlah paper per kategori utama\n",
        "print(df['main_category'].value_counts())\n",
        "\n",
        "# Filter untuk kategori tertentu\n",
        "ml_papers = df[df['main_category'].str.contains('cs.AI', na=False)]"
      ],
      "metadata": {
        "id": "29vFZB9iXTGX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41d6df11-fe23-49df-af31-396d776089e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main_category\n",
            "cs.CV              122\n",
            "cs.LG               66\n",
            "quant-ph            42\n",
            "cs.CL               32\n",
            "cs.CR               30\n",
            "                  ... \n",
            "physics.acc-ph       1\n",
            "physics.ao-ph        1\n",
            "cs.CE                1\n",
            "cs.DM                1\n",
            "cond-mat.dis-nn      1\n",
            "Name: count, Length: 99, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lihat trend publikasi per tahun\n",
        "yearly_counts = df['timestamp'].dt.year.value_counts().sort_index()\n",
        "display(yearly_counts)"
      ],
      "metadata": {
        "id": "6EhBGkwzXlp0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "1034493d-3368-4405-b365-2d0e74df783d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "timestamp\n",
              "2024    755\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2024</th>\n",
              "      <td>755</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk scikit-learn"
      ],
      "metadata": {
        "id": "MK4qWrX5YJ5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7739f3f5-eeaf-406b-911b-448cdd014f10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk pandas numpy scikit-learn streamlit"
      ],
      "metadata": {
        "id": "Z3T0tH7lZcmP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd71cee5-4913-488f-fb2b-dadc88f7ef06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.41.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.25.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Downloading streamlit-1.41.0-py2.py3-none-any.whl (23.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.41.0 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "MJBZ9e5jZhZR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8173f036-53a2-4162-bd9a-a41940857573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls *.csv"
      ],
      "metadata": {
        "id": "5nGPE-_WYPo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a870b57-df0d-4732-bd9b-f56b782d39f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "arxiv_cs_papers_20241211_0331.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "# Download nltk data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Membersihkan dan memproses teks\n",
        "    \"\"\"\n",
        "    if isinstance(text, str):\n",
        "        # Lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Hapus karakter khusus dan angka\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "        # Tokenisasi\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        # Hapus stopwords\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "        return ' '.join(tokens)\n",
        "    return ''\n",
        "\n",
        "def create_feature_matrix(df):\n",
        "    \"\"\"\n",
        "    Membuat matriks fitur dari data paper\n",
        "    \"\"\"\n",
        "    # Pastikan kolom yang diperlukan ada\n",
        "    required_columns = ['title', 'summary', 'categories']\n",
        "    for col in required_columns:\n",
        "        if col not in df.columns:\n",
        "            df[col] = ''\n",
        "\n",
        "    # Gabungkan fitur\n",
        "    df['combined_features'] = df['title'] + ' ' + df['summary'] + ' ' + df['categories']\n",
        "\n",
        "    # Preprocessing\n",
        "    print(\"Melakukan preprocessing teks...\")\n",
        "    df['processed_features'] = df['combined_features'].apply(preprocess_text)\n",
        "\n",
        "    # Buat TF-IDF matrix\n",
        "    print(\"Membuat TF-IDF matrix...\")\n",
        "    tfidf = TfidfVectorizer(max_features=5000)  # Batasi jumlah fitur\n",
        "    feature_matrix = tfidf.fit_transform(df['processed_features'])\n",
        "\n",
        "    return feature_matrix, tfidf\n",
        "\n",
        "def calculate_similarity_matrix(feature_matrix):\n",
        "    \"\"\"\n",
        "    Menghitung matriks similarity antar paper\n",
        "    \"\"\"\n",
        "    print(\"Menghitung similarity matrix...\")\n",
        "    return cosine_similarity(feature_matrix)\n",
        "\n",
        "def get_recommendations(paper_idx, similarity_matrix, df, n_recommendations=5):\n",
        "    \"\"\"\n",
        "    Mendapatkan rekomendasi paper\n",
        "    \"\"\"\n",
        "    paper_similarities = similarity_matrix[paper_idx]\n",
        "    similar_indices = paper_similarities.argsort()[::-1][1:n_recommendations+1]\n",
        "\n",
        "    recommendations = pd.DataFrame({\n",
        "        'title': df.iloc[similar_indices]['title'],\n",
        "        'authors': df.iloc[similar_indices]['authors'],\n",
        "        'similarity_score': paper_similarities[similar_indices],\n",
        "        'link': df.iloc[similar_indices]['link']\n",
        "    })\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "def prepare_recommendation_system(filename):\n",
        "    \"\"\"\n",
        "    Menyiapkan sistem rekomendasi\n",
        "    \"\"\"\n",
        "    print(f\"Membaca file: {filename}\")\n",
        "    try:\n",
        "        # Baca file CSV\n",
        "        df = pd.read_csv(filename)\n",
        "        print(f\"Berhasil membaca file dengan {len(df)} baris data\")\n",
        "\n",
        "        # Bersihkan data\n",
        "        df = df.dropna(subset=['title'])\n",
        "        print(f\"Data setelah dibersihkan: {len(df)} baris\")\n",
        "\n",
        "        # Buat feature matrix\n",
        "        feature_matrix, tfidf = create_feature_matrix(df)\n",
        "        print(f\"Feature matrix shape: {feature_matrix.shape}\")\n",
        "\n",
        "        # Hitung similarity matrix\n",
        "        similarity_matrix = calculate_similarity_matrix(feature_matrix)\n",
        "        print(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n",
        "\n",
        "        # Simpan hasil preprocessing\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "        processed_filename = f'processed_papers_{timestamp}.csv'\n",
        "        similarity_filename = f'similarity_matrix_{timestamp}.npy'\n",
        "\n",
        "        df.to_csv(processed_filename, index=False)\n",
        "        np.save(similarity_filename, similarity_matrix)\n",
        "\n",
        "        print(f\"\\nHasil preprocessing disimpan di:\")\n",
        "        print(f\"- {processed_filename}\")\n",
        "        print(f\"- {similarity_filename}\")\n",
        "\n",
        "        return df, feature_matrix, similarity_matrix, tfidf\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File {filename} tidak ditemukan\")\n",
        "        print(\"Pastikan nama file sesuai dengan hasil scraping sebelumnya\")\n",
        "        return None, None, None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "# Jalankan sistem\n",
        "def main():\n",
        "    # Cari file CSV yang ada\n",
        "    import glob\n",
        "    csv_files = glob.glob('arxiv*.csv')\n",
        "\n",
        "    if not csv_files:\n",
        "        print(\"Tidak ada file CSV yang ditemukan\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    # Gunakan file terbaru\n",
        "    latest_file = max(csv_files)\n",
        "    print(f\"Menggunakan file: {latest_file}\")\n",
        "\n",
        "    # Preprocessing\n",
        "    df_processed, feature_matrix, similarity_matrix, tfidf = prepare_recommendation_system(latest_file)\n",
        "\n",
        "    if df_processed is not None:\n",
        "        # Tampilkan contoh rekomendasi\n",
        "        print(\"\\nContoh rekomendasi untuk paper pertama:\")\n",
        "        recommendations = get_recommendations(0, similarity_matrix, df_processed)\n",
        "        display(recommendations)\n",
        "\n",
        "        print(\"\\nStatistik data:\")\n",
        "        print(f\"Total paper: {len(df_processed)}\")\n",
        "        print(f\"Jumlah fitur: {feature_matrix.shape[1]}\")\n",
        "        if 'main_category' in df_processed.columns:\n",
        "            print(\"\\nDistribusi kategori:\")\n",
        "            print(df_processed['main_category'].value_counts().head())\n",
        "\n",
        "    return df_processed, feature_matrix, similarity_matrix, tfidf\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df_processed, feature_matrix, similarity_matrix, tfidf = main()"
      ],
      "metadata": {
        "id": "-qbk4SADX-1R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44bb4f51-95e9-41e1-ac0c-ae71b1bbe05c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Menggunakan file: arxiv_cs_papers_20241211_0331.csv\n",
            "Membaca file: arxiv_cs_papers_20241211_0331.csv\n",
            "Berhasil membaca file dengan 755 baris data\n",
            "Data setelah dibersihkan: 755 baris\n",
            "Melakukan preprocessing teks...\n",
            "Error: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_processed, feature_matrix, similarity_matrix, tfidf = main()"
      ],
      "metadata": {
        "id": "niEJBuZJYuE8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9043d942-1be8-4b01-bb1e-0b8b3d5b3de8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Menggunakan file: arxiv_cs_papers_20241211_0331.csv\n",
            "Membaca file: arxiv_cs_papers_20241211_0331.csv\n",
            "Berhasil membaca file dengan 755 baris data\n",
            "Data setelah dibersihkan: 755 baris\n",
            "Melakukan preprocessing teks...\n",
            "Error: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Untuk paper tertentu\n",
        "if df_processed is not None:\n",
        "    # Tampilkan judul paper yang tersedia\n",
        "    print(\"Daftar 5 paper pertama:\")\n",
        "    for idx, title in enumerate(df_processed['title'][:5]):\n",
        "        print(f\"{idx}: {title}\")\n",
        "\n",
        "    # Pilih paper untuk mendapatkan rekomendasi\n",
        "    paper_idx = 0  # Ganti dengan indeks yang diinginkan\n",
        "    recommendations = get_recommendations(paper_idx, similarity_matrix, df_processed)\n",
        "    display(recommendations)"
      ],
      "metadata": {
        "id": "oCdZZoVKYv1L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}